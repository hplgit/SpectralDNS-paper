\documentclass[11pt, oneside]{article}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ptex2tex, minted}
\usepackage{xcolor}
\usepackage{cancel}
\usepackage{caption}
\usepackage{subfigure}
\subfiglabelskip=0pt

\definecolor{gray}{gray}{0.97}
\colorlet{commentcolour}{green!50!black}
\colorlet{stringcolour}{red!60!black}
\colorlet{keywordcolour}{magenta!90!black}
\colorlet{exceptioncolour}{yellow!50!red}
\colorlet{commandcolour}{blue!60!black}
\colorlet{numpycolour}{blue!60!green}
\colorlet{literatecolour}{magenta!90!black}
\colorlet{promptcolour}{green!50!black}
\colorlet{specmethodcolour}{violet}
\colorlet{indendifiercolour}{green!70!white}

%\newcommand{\codetitlestyle}[1]{\small\textit{#1}\hspace{0.1cm}}
\newcommand{\belowtitleskip}{2pt}%\smallskipamount}
%\newcommand{\captionposition}{t}

%\newcommand{\mmo}[1]{\emph{#1}}

%\renewcommand{\ttdefault}{pcr}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[top=3cm,bottom=4cm,left=3cm,right=3.2cm,asymmetric]{geometry}

\lstset{
numbers=none,
aboveskip=1ex,
belowskip=1ex,
basicstyle=\ttfamily\footnotesize,
}

\lstdefinestyle{pythonstyle}{
%%\lstset{
%%keepspaces=true,
language=python,
showtabs=true,
tab=,
tabsize=2,
basicstyle=\ttfamily\footnotesize,%\setstretch{.5},
stringstyle=\color{stringcolour},
showstringspaces=false,
alsoletter={1234567890},
otherkeywords={\ , \}, \{, \%, \&, \|},
keywordstyle=\color{keywordcolour}\bfseries,
emph={and,break,class,continue,def,yield,del,elif ,else,%
except,exec,finally,for,from,global,if,import,in,%
lambda,not,or,pass,print,raise,return,try,while,assert},
emphstyle=\color{blue}\bfseries,
emph={[2]True, False, None},
emphstyle=[2]\color{keywordcolour},
emph={[3]object,type,isinstance,copy,deepcopy,zip,enumerate,reversed,list,len,dict,tuple,xrange,append,execfile,real,imag,reduce,str,repr},
emphstyle=[3]\color{commandcolour},
emph={Exception,NameError,IndexError,SyntaxError,TypeError,ValueError,OverflowError,ZeroDivisionError},
emphstyle=\color{exceptioncolour}\bfseries,
%upquote=true,
morestring=[s]{"""}{"""},
morestring=[s]{'''}{'''},
commentstyle=\color{commentcolour}\slshape,
emph={[4]1, 2, 3, 4, 5, 6, 7, 8, 9, 0,ode, fsolve, sqrt, exp, sin, cos, arccos, pi, array, norm, solve,float,complex, dot, arange, isscalar, max, sum, flatten, shape, reshape, find, any, all, abs, plot, linspace, legend, quad, polyval,polyfit, hstack,vector, concatenate,vstack,column_stack,empty,zeros,ones,rand,vander,grid,pcolor,eig,eigs,eigvals,svd,qr,tan,det,logspace,roll,min,mean,cumsum,cumprod,diff,vectorize,lstsq,cla,eye,xlabel,ylabel,squeeze},
emphstyle=[4]\color{commandcolour},
emph={[5]__init__,__add__,__mul__,__div__,__sub__,__call__,__getitem__,__setitem__,__eq__,__ne__,__nonzero__,__rmul__,__radd__,__repr__,__str__,__get__,__truediv__,__pow__,__name__,__future__,__all__},
emphstyle=[5]\color{specmethodcolour},
emph={[6]assert,range,yield},
emphstyle=[6]\color{keywordcolour}\bfseries,
emph={[7]def, return, and, print},
emphstyle=[7]\color{black}\bfseries,
% emph={[7]self},
% emphstyle=[7]\bfseries,
literate=*%
%{:}{{\literatecolour:}}{1}%
%{=}{{\literatecolour=}}{1}%
%{-}{{\literatecolour-}}{1}%
%{+}{{\literatecolour+}}{1}%
%{*}{{\literatecolour*}}{1}%
{/}{{\literatecolour/}}{1}%
{!}{{\literatecolour!}}{1}%
%{(}{{\literatecolour(}}{1}%
%{)}{{\literatecolour)}}{1}%
%{[}{{\literatecolour[}}{1}%
%{]}{{\literatecolour]}}{1}%
{<}{{\literatecolour<}}{1}%
{>}{{\literatecolour>}}{1}%
{>>>}{{\textcolor{promptcolour}{>>>}}}{1}%
,%
breaklines=true,
breakatwhitespace= true,
xleftmargin=\framemargin,
xrightmargin=\framemargin,
aboveskip=1ex,
belowskip=1ex,
frame=trbl, %trbl
numbers=none,
%frameround=tttt,
rulecolor=\color{black!40},
%framexleftmargin=\framemargin,
%framextopmargin=.1ex,
%framexbottommargin=.1ex,
%framexrightmargin=\framemargin,
%framexleftmargin=1mm, framextopmargin=1mm, frame=shadowbox, rulesepcolor=\color{blue},#1
%frame=tb,
%backgroundcolor=\color{yellow!10}
backgroundcolor=\color{gray}
%}
}

\newcommand{\inpyth}{\lstinline[style=pythonstyle, basicstyle=\ttfamily]} %[]%

\lstnewenvironment{inpython}[1][]{
\lstset{style=pythonstyle, frame=trbl, belowcaptionskip=\belowtitleskip}
}{}

\newcommand{\includecode}[2][py]{\lstinputlisting[caption=#2,label=list:#2,style=pythonstyle,
float=!htpb]{#2}}

\newcommand{\hpl}[1]{({\bf hpl comment:} \emph{#1})}
\bibliographystyle{plain}

\title{Massively Parallel Python Implementation of a Pseudo-Spectral DNS Code for Turbulent Flow}
\author{Mikael Mortensen and Hans Petter Langtangen}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\section{Abstract}

\section{Introduction}
%\subsection{}
Direct Numerical Simulations (DNS) is a term reserved for computer simulations of turbulent flows that are fully resolved in both time and space. DNS are usually conducted using numerical methods of such high quality that numerical dispersion and diffusion errors are negligible compared to their actual physical counterparts. To this end, DNS has historically been carried out with extremely accurate and efficient spectral methods, and in the fluid dynamics community DNS enjoys  today the same status as carefully conducted experiments. DNS can provide detailed and highly reliable data not possible to extract from experiments, which in recent years have driven a number of discoveries regarding the very nature of turbulence. The present paper presents a new, computationally attractive tool for performing DNS, realized by recent programming technologies.

Because of the extremely heavy number crunching implied by DNS,
researchers aim at highly optimized implementations running on
massively parallel computing platforms. The largest known DNS
simulations performed today are using hundreds of billions of degrees
of freedom. Normally, this demands a need for developing tailored, hand-tuned
codes in what we here call low-level languages: Fortran, C or C++. Few
DNS codes are openly available and easily accessible to the public and
the common fluid mechanics researcher. Some exceptions are hit-3d
(Fortran90) \cite{hit-3d}, Philofluid (Fortran) \cite{philofluid},
Tarang (C++) \cite{tarang}, and Turbo (Fortran90)
\cite{turbo}. However, the user interfaces to these codes are not highly developed and it is both challenging and time consuming for a user to to modify or extend the codes to satisfy their own needs. This is usually the nature of low-level codes.

It is a clear trend in computational sciences over the last two decades
that researchers tend to move from low-level languages to high-level languages
like Matlab, Python, R, and IDL. The experience is that implementations
in high-level languages are faster to develop, easier to test,
easier to maintain, and they
reach a much wider audience because the codes are compact and readable.
The downside has been the decreased computational
efficiency of high-level languages and in particular their lack of
suitability for massively parallel computing. In a field like computational
fluid dynamics, this argument has been a show stopper for wide use
of high-level languages. However, a language like Python has capabilities
today for providing short and quick implementations that compete with
tailored implementations in low-level languages up to thousands of processors.
This fact is not well known, and the purpose of this paper is to
demonstrate such a result for DNS and show the technical implementation
details that are needed.

%Because of the massive amounts of number crunching involved, DNS solvers are usually implemented in high entry-level, low-level languages like Fortran/Fortran90 or C/C++.

Python is a language that over the last two decades has grown very popular in the scientific computing community. A wide range of well established, ``gold standard'' scientific libraries in Fortran and C have been wrapped in Python, making them directly accessible just as commands in MATLAB. There is little overhead in calling low-level Fortran and C/C++functions from Python, and the computational speed obtained in a few lines of code may easily compete with hundreds of compiled lines of Fortran or C code. It is important new knowledge in the CFD community if flow codes can be developed with comfort and ease in Python without sacrificing much computational efficiency.

There are already several examples on successful use of Python for
high-performance parallel scientific computing. The sophisticated
finite element framework FEniCS \cite{fenics} is written mainly in
C++, but most application developers are writing FEniCS-based solvers
directly in Python, never actually finding themselves in need of
writing longer C++ code and firing up a compiler. For large scale
applications the devloped Python solvers are usually equally fast as
their C++ counterparts, because most of the computing time is usually
spent within the low-level wrapped C++ functions that perform the
costly linear algebra operations \cite{Mortensen2015}. \hpl{Should add parallel FEniCS numbers.}
GPAW \cite{gpaw05} is a code devoted
to electronic structure calculations, written as a combination of
Python and C. GPAW solvers written in Python have been shown to scale
well for thousands of processors.  The PETSc project \cite{petsc-web-page} is a major
provider of linear algebra to the open source community. PETSc was
developed in C, but through the package \texttt{petsc4py} almost all
routines may be set up and called from Python. PyClaw \cite{ketcheson2012}
is another good example, providing a compact, powerful, and intuitive
Python interface to the algorithms within the Fortran codes Clawpack
and SharpClaw. PyClaw is parallelised through PETSc and has been shown
to scale well up to 65,000 cores.

The ability of Python to wrap low-level, computationally highly efficient Fortran and C/C++ libraries for various applications is today well known, appreciated, and utilized by many. A lesser known fact is that basic Python modules like \texttt{numpy}, used for linear algebra and array manipulations, and \texttt{mpi4py}, which wraps (nearly) the entire MPI library, may be used directly to develop, from scratch, high performance solvers that run at speeds comparable to the very best implementations in low-level codes. A general misconception seems to be that Python may be used for fast prototyping and post-processing, as MATLAB, but that serious high-performance computing on parallel platforms require reimplementations in Fortran, C or C++. In this paper, we conquer this misconception: The only real requirement for developing a fast pure \texttt{numpy}/\texttt{mpi4py} solver is that all array manipulations are performed using vectorization (that call underlying BLAS or LAPACK backends) such that explicit for loops over long arrays in Python are avoided. The \texttt{mpi4py} module in turn provides a message passing interface for \texttt{numpy} arrays at communication speeds very close to pure C code.

The major objective of this work is to explain a novel implementation of an excellent research tool (DNS) aimed at a wide audience. To this end, we i) show how a complete pseudo-spectral DNS solver can be written from scratch in Python using less than 100 lines of compact, very readable code, and ii) show that these 100 lines of code can run at speeds comparable to its low-level counterpart in hand-written C++ code. To establish scaling and benchmark results, we have run the codes on SHAHEEN, a massively parallel BlueGene/P machine at the KAUST Supercomputing Laboratory.

\section{The Navier-Stokes equations in spectral space}
Our DNS implementation is based on a pseudo-spectral Galerkin method \cite{canuto1987} for the spatial discretization. The Navier-Stokes equations are first cast in rotational form

\begin{align}
 \frac{\partial \bm{u}}{\partial t} - \bm{u} \times \bm{\omega}   &= \nu \nabla^2 \bm{u} - \nabla{P}, \label{eq:NS} \\
 \nabla \cdot \bm{u} &= 0, \\
 \bm{u}(\bm{x}+2\pi \bm{e}^i, t) &= \bm{u}(\bm{x}, t), \quad \text{for }\, i=1,2,3,\\
 \bm{u}(\bm{x}, 0) &= \bm{u}_0(\bm{x})
\end{align}
where $\bm{u}(\bm{x}, t)$ is the velocity vector, $\bm{\omega}=\nabla \times \bm{u}$ the vorticity vector, $\bm{e}^i$ the Cartesian unit vectors, and the modified pressure $P=p+\bm{u}\cdot \bm{u}/2$, where $p$ is the regular pressure normalized by the constant density. The equations are periodic in all three spatial directions. If all three directions now are discretized uniformely in space using a structured computational mesh with $N$ points in each direction, the mesh points can be represented as
\begin{align}
{x}_j &= \frac{2\pi j}{N}, \quad j=0,\ldots, N-1,\\
\bm{x}^i_j &= x_j\bm{e}^i, \quad j=0,\ldots, N-1, \,\,i=1,2,3.
\label{eq:realmesh}
\end{align}
All variables may be transformed from the physical mesh $\bm{x}^i_j$ to a discrete and bounded Fourier wavenumber mesh using three-dimensional discrete Fourier transforms. Each point in the physical mesh will take part in three consecutive transformations, one for each periodic direction. The first transformed direction (arbitrary which one) is real and the remaining two are complex valued. There are in total $N^2$ real transforms of length $N$ and $2(N/2+1)N$ complex transforms of length $N$. The transforms in the three directions are performed sequentially. The first real transform along one line in the $z$-direction reads
\begin{align}
\mathcal{F}_{k_z}(\bm{u}) = \hat{\bm{u}}_{k_z}(t) &= \frac{2\pi}{N}\sum_{j=0}^{N-1}{\bm{u}({x}_j, t)}e^{-i k_z x_j}, \quad k_z=-N/2+1, \ldots, N/2,
\end{align}
with the inverse transform
\begin{align}
\mathcal{F}^{-1}_{j} (\hat{\bm{u}}) =\bm{u}(x_j, t) &= \frac{1}{2\pi}\sum_{k_z=-N/2+1}^{N/2}\hat{\bm{u}}_{k_z}(t)e^{i k_z {x}_j}, \quad j=0, \ldots, N-1.
\end{align}
Note that $i$ is used both as a spatial counter and as the imaginary unit. The
meaning should be clear from the context.

The transform is performed along all $N^2$ lines on the cubic mesh in the $z$-direction. To simplify notation, the complete three-dimensional transform for the entire mesh used in this work is denoted as
\begin{equation}
\mathcal{F}_{\bm{k}}(\bm{u}) = \hat{\bm{u}}_{\bm{k}}(t) = \mathcal{F}_{k_x}\left(\mathcal{F}_{k_y}\left(\mathcal{F}_{k_z}(\bm{u})\right)\right), \quad \bm{k}=(k_x, k_y, k_z).
\end{equation}
Note, however, that the order is arbitrary except from the data layout in memory. Similarily, using $i,j,k$ for the three Cartesian directions $x,y,z$, the inverse transform is defined as
\begin{equation}
\mathcal{F}^{-1}_{\bm{x}}(\hat{\bm{u}}) = \bm{u}(\bm{x}, t) = \mathcal{F}^{-1}_{k}\left(\mathcal{F}^{-1}_{j}\left(\mathcal{F}^{-1}_{i}(\hat{\bm{u}})\right)\right), \quad \bm{x} = (x, y, z),
\end{equation}
where the inverse transforms are taken in the opposite order of the forward transforms.

By taking the Fourier transform of the Navier-Stokes equations and subsequently the analythical spatial derivatives in spectral space, we obtain a system of ordinary differential equation for $\hat{\bm{u}}_{\bm{k}}$, and the continuity equation reduces to an orthogonal inner product:
\begin{align}
 \frac{d\hat{\bm{u}}_{\bm{k}}}{d t} - \widehat{( \bm{u} \times \bm{\omega})}_{\bm{k}} &= - \nu |\bm{k}|^2  \hat{\bm{u}}_{\bm{k}} - i \bm{k} \hat{P}_{\bm{k}}, \label{eq:NSf} \\
 i \bm{k} \cdot \hat{\bm{u}}_{\bm{k}} &= 0.
\end{align}
The pressure may be eliminated by taking the divergence of (\ref{eq:NS}), or equivalently by dotting the transformed (\ref{eq:NSf}) by $i \bm{k}$ and rearranging such that
\begin{equation}
\hat{P}_{\bm{k}} = -i \frac{\bm{k} \cdot \widehat{( \bm{u} \times \bm{\omega})}_{\bm{k}} }{|\bm{k}|^2}.
\label{eq:mod_pressure}
\end{equation}
Inserting for the pressure in (\ref{eq:NSf}), the final equation to solve for each transformed velocity vector is thus
\begin{equation}
 \frac{d\hat{\bm{u}}_{\bm{k}}}{d t}  = \widehat{( \bm{u} \times \bm{\omega})}_{\bm{k}} - \nu |\bm{k}|^2  \hat{\bm{u}}_{\bm{k}} - \bm{k} \frac{\bm{k} \cdot \widehat{( \bm{u} \times \bm{\omega})}_{\bm{k}} }{|\bm{k}|^2}. \label{eq:NSfinal}
\end{equation}
Note that the transformed velocity components are coupled through the nonlinear convection term and the eliminated pressure.

The pseudo-spectral label arises from the treatment of the convective term, which is computed by first transforming the velocity and vorticity to physical space, performing the cross product, and then transforming the vector ${(\bm{u}  \times  \bm{\omega})}$  back to Fourier space. The operation requires 2 inverse transforms (velocity and vorticity) and 1 forward transform for each of the three vector components, 9 all together. Note that this is the only operation that requires MPI communication and it is typically the most computationally extensive part of a pseudo-spectral DNS solver.

The time integration of (\ref{eq:NSfinal}) is performed explicitly using a fourth-order Runge-Kutta method, the Forward Euler method or a second-order Adams-Bashforth method. The details are left out here, but all algorithms simply need a function that returns the right hand side of (\ref{eq:NSfinal}).

\section{Implementation}

We have implemented the pseudo-spectral discretization of the Navier-Stokes equations, as described in the previous section, in high-level Python code. It is strongly remarked that this Python code is not simply a wrapper of a low-level, high-performance flow solver in Fortran, C or C++. The entire code is implemented directly in Python: the mesh, the MPI domain decomposition, and the time integrators. We are only making use of wrappers for FFT, something that is also done by the majority of low-level flow solvers anyway. The current Python implementation may, as such, be used as a working prototype for a complete low-level implementation in Fortran, C or C++.

The Python solver makes extensive use of the \texttt{numpy} and \texttt{mpi4py} packages, but if the \texttt{pyfftw} module has been installed, it will be used to perform the FFT instead of \texttt{numpy.fft}.

\begin{python}
from numpy import *
from numpy.fft import fftfreq, fft, ifft, rfft, irfft
try:
    from mpi.wrappyfftw import *
except ImportError:
    pass # Rely on numpy.fft routines
from mpi4py import MPI
comm = MPI.COMM_WORLD
num_processes = comm.Get_size()
rank = comm.Get_rank()
\end{python}
Importing \texttt{MPI} from \texttt{mpi4py} initializes the MPI communicator. Two different strategies, slab and pencil, have been implemented for the MPI parallel domain decomposition. However, since communication only enters through the FFTs, there is very little difference between a serial code and a parallel one. Therefore, we first present a serial version of the code.

\subsection{Serial version of code}
The computational mesh is in physical space a structured uniform cube $[0, 2\pi]^3$, where each direction is divided into $N$ uniform intervals, where $N=2^M$ for a postive integer $M$. Any different size of the box may be easily implemented through scaling. The mesh according to (\ref{eq:realmesh}) is represented in Python as

\begin{python}
M = 6
N = 2**M
L = 2*pi
X = mgrid[:N, :N, :N].astype(float)*L/N
\end{python}
The matrix \inpyth{X} has dimensions \inpyth{(3, N, N, N)}. Since the Navier Stokes equations are solved in Fourier space, the physical space is only used to compute the convection plus to do post processing. The mesh \inpyth{X} is typically used for initialization and otherwise not needed (and may therefore be deleted to save memory). In parallel mode, \inpyth{X} will be split up and divided between the processors.  Note that the solver may be operated in either single or double precision mode, and that \inpyth{float} in our code is a placeholder for either one of the \inpyth{numpy} datatypes \inpyth{float32} or \inpyth{float64}, depending on settings.

The velocity field to be transformed is real, and the discrete Fourier transform of a real sequence has the property that $\hat{\bm{u}}_k = \hat{\bm{u}}_{N-k}^*$, where $^*$ denotes the complex conjugate. As such, it is sufficient to use $N/2+1$ Fourier coefficients in the first transformed direction, leading to a transformed wavenumber mesh of size $(N/2+1)N^2$. The odd number of wavenumbers does not lead to any issues for the serial version of the code or for the slab decomposition. However, for the pencil decomposition it requires special attention, see Sec. ??. 

In our code the real transform is taken in the final $z$ direction and the wavenumbers $\bm{k}=(k_x, k_y, k_z)$ stored on the transformed mesh thus has ordering as used by the FFT routines provided by \texttt{numpy.fft} or \texttt{pyfftw}:

\begin{align}
  \bm{k} = [&(0, \ldots, N/2-1, -N/2, -N/2+1, \ldots, -1), \notag \\
   &(0, \ldots, N/2-1, -N/2, -N/2+1, \ldots, -1),  \notag \\
  &(0, \ldots, N/2-1, N/2)].
\end{align}
The wavenumber mesh is implemented as

\begin{python}
Nf = N/2+1
kx = ky = fftfreq(N, 1./N).astype(int)
kz = kx[:Nf].copy(); kz[-1] *= -1
K = array(meshgrid(kx, ky, kz, indexing='ij'), dtype=int)
K2 = sum(K*K, 0, dtype=int)
K_over_K2 = K.astype(float) / where(K2 == 0, 1, K2).astype(float)
\end{python}
where \inpyth{fftfreq(N, 1./N)} is a function that creates the wavenumbers $(0, \ldots, N/2-1, -N/2, -N/2+1, \ldots, -1)$. The dimesions of the matrices are \inpyth{(3, N, N/2+1, N)} for \texttt{K}, \inpyth{(N, N/2+1, N)} for \texttt{K2} and \inpyth{(3, N, N/2+1, N)} for \texttt{K\_over\_K2}, and these matrices represent $\bm{k}$, $|\bm{k}|^2$ and $\bm{k}/|\bm{k}|^2$, respectively. The last two matrices are precomputed for efficiency.

The velocity, curl and pressure are similarily stored in structured numpy arrays

\begin{python}
U     = empty((3, N, N, N),  dtype=float)
U_hat = empty((3, N, N, Nf), dtype=complex)
P     = empty((N, N, N),     dtype=float)
P_hat = empty((N, N, Nf),    dtype=complex)
curl  = empty((3, N, N, N),  dtype=float)
\end{python}
Here \inpyth{hat} denotes a transformed variable. To transform between, e.g., \inpyth{U} and \inpyth{U_hat}, calls to FFT routines are required. The three dimensional FFT and its inverse are implemended in Python functions as shown below.

\begin{python}
def fftn_mpi(u, fu):
    """FFT of u in three directions."""
    if num_processes == 1:                # Scalar version
        fu[:] = rfftn(u, axes=(0,1,2))
    return fu

def ifftn_mpi(fu, u):
    """Inverse FFT of fu in three directions."""
    if num_processes == 1:                # Scalar version
        u[:] = irfftn(fu, axes=(0,1,2))
    return u

# Usage
U_hat = fftn_mpi(U, U_hat)
U = ifftn_mpi(U_hat, U)
\end{python}
For high performance, it is key that the Python code relies on \emph{in-place}
modifications of pre-allocated arrays to avoid unnecessary allocation of
large temporary arrays (which often arises from basic \texttt{numpy} code).
Each of the functions above takes the result array (\texttt{U\_hat} or
\texttt{U}) as argument, fill this array with values and returns the
array to the calling code. A commonly applied convention in
Python is to return all result objects from functions as this only involves
transfer of references and no copying of data.

We also remark that the three consecutive transforms performed by \inpyth{rfftn/irfftn} are actually using one real transform along the $z$-direction and two complex transforms in the remaining two directions. Also note that the simple \texttt{numpy/pyfftw} wrapped functions \inpyth{rfftn/irfftn} may only be used in single processor mode, and the MPI implementation is detailed in Sections
\ref{slab1D} and \ref{pencil2D}.

The convection term requires a transform from Fourier to physical space where the cross product $\bm{u} \times \bm{\omega}$ is carried out. The curl in Fourier space is
\begin{equation}
\mathcal{F}_{\bm{k}}(\nabla \times \bm{u}) = \hat{\bm{\omega}}_{\bm{k}} = i \bm{k} \times \hat{\bm{u}}_{\bm{k}}.
\end{equation}
We can now compute the curl in physical space through $\bm{\omega} = \mathcal{F}_{\bm{x}}^{-1}(\hat{\bm{\omega}})$. The convection term may thus be computed as
\begin{equation}
\widehat{( \bm{u} \times \bm{\omega})}_{\bm{k}} = \mathcal{F}_{\bm{k}}(\bm{u} \times \bm{\omega}) = \mathcal{F}_{\bm{k}} (\mathcal{F}^{-1}_{\bm{x}}(\hat{\bm{u}}) \times \mathcal{F}^{-1}_{\bm{x}}(\hat{\bm{\omega}})).
\end{equation}
The Python functions for the curl and cross products are

\begin{python}
def Cross(a, b, c):
    c[0] = fftn_mpi(a[1]*b[2]-a[2]*b[1], c[0])
    c[1] = fftn_mpi(a[2]*b[0]-a[0]*b[2], c[1])
    c[2] = fftn_mpi(a[0]*b[1]-a[1]*b[0], c[2])
    return c

def Curl(a, c):
    c[2] = ifftn_mpi(1j*(K[0]*a[1]-K[1]*a[0]), c[2])
    c[1] = ifftn_mpi(1j*(K[2]*a[0]-K[0]*a[2]), c[1])
    c[0] = ifftn_mpi(1j*(K[1]*a[2]-K[2]*a[1]), c[0])
    return c
\end{python}

The convection term in (\ref{eq:NSfinal}) is dealiased using the 2/3-rule and dealiasing is simply achieved by an elementwise multiplication of a convection matrix \inpyth{dU[3, N, N/2+1, N]} with a matrix \inpyth{dealias[N, N/2+1, N]}, which is zero where the wavenumbers are larger than 2/3 of the Nyquist mode and unity otherwise. Note that the dimensions of \inpyth{dU} and \inpyth{dealias} differ in the first index since \inpyth{dU} contains contributions for all three vector components. However, through automatic broadcasting, \texttt{numpy} realizes that the last three dimensions are the same and as such all three components of \inpyth{dU} (i.e.,  \inpyth{dU[0]}, \inpyth{dU[1]} and  \inpyth{dU[2]}) are multiplied elementwise with the same matrix \inpyth{dealias}.

\begin{python}
# Declare some matrices and parameters
kmax_dealias = N/3.
dU = empty((3, N, Nf, N), dtype=complex)  # Holds right-hand side
dealias = array((abs(K[0]) < kmax_dealias)*(abs(K[1]) < kmax_dealias)*
                (abs(K[2]) < kmax_dealias), dtype=bool)
dt = 0.01    # Time step
nu = 0.001   # Viscosity

def ComputeRHS(dU, rk):
    if rk > 0: # For rk=0 the correct values are already in U
        for i in range(3):
            U[i] = ifftn_mpi(U_hat[i], U[i])

    # Compute convective term and place in dU
    curl = Curl(U_hat, curl)
    dU = Cross(U, curl, dU)

    # Dealias the nonlinear convection
    dU *= dealias

    # Compute pressure (to get actual modified pressure multiply by 1j)
    P_hat[:] = sum(dU*K_over_K2, 0, out=P_hat)

    # Subtract pressure gradient
    dU -= P_hat*K

    # Subtract viscous term
    dU -= nu*K2*U_hat

    return dU
\end{python}
The simple Forward Euler integrator is now
\begin{python}
t = 0        # Physical time
T = 1.0      # End time
while t <= T:
    t += dt
    U_hat += ComputeRHS(dU, 0)*dt
\end{python}

\subsection{1D Slab decomposition}
\label{slab1D}

To run the code on several processors using MPI, the mesh and the data structures need to be split up. The most popular strategy in the litterature is the ``slab'' decomposition, where each CPU is assigned responsibility for a certain number of complete 2D planes (slices). In other words, just one of the three indices $(i,j,k)$ is split up and divided amongst the CPUs. The major drawback of the slab decomposition strategy is that the number of CPUs must be smaller than or equal to $N$ for a cubic mesh of size $N^3$. Note that the MPI implementation in FFTW makes use of the slab decomposition strategy, but there is currently no interface from Python to these MPI routines.

The real and wavenumber meshes for the slab decomposition are implemented as

\begin{python}
Np = N / num_processes
X = mgrid[rank*Np:(rank+1)*Np, :N, :N].astype(float)*L/N
K = array(meshgrid(kx, ky[rank*Np:(rank+1)*Np], kz, 
                   indexing='ij'), dtype=int)
\end{python}
In general, using \inpyth{num_processes} CPUs, each CPU gets responsibility for \inpyth{Np = N/num_processes} slices and the physical mesh is simply decomposed along the first index. The wavenumber mesh, on the other hand, is split along the second index. The reason for this choice is that the $k_y$ direction is the last direction to be transformed by the three consecutive FFTs and for this operation the data structures need to be aligned in the $k_x$ direction.

The MPI decomposition is illustrated in Fig. \ref{fig:Slabdecomp} for the case of a physical mesh of size $8^3$ divided amongst 4 CPUs. The entire 3D parallel FFT may be implemented with 5 lines of Python code:

\begin{python}
def fftn_mpi(u, fu):
    """FFT in three directions using MPI."""
    Uc_hatT[:] = rfft2(u, axes=(2,1))
    for i in range(num_processes):
        U_mpi[i] = Uc_hatT[:, :, i*Np:(i+1)*Np]
    comm.Alltoall([U_mpi, mpitype], [fu, mpitype])
    fu[:] = fft(fu, axis=0)
    return fu

def ifftn_mpi(fu, u):
    """Inverse FFT in three directions using MPI.
       Need to do ifft in reversed order of fft."""
    Uc_hat[:] = ifft(fu, axis=0)
    comm.Alltoall([Uc_hat, mpitype], [U_mpi, mpitype])
    for i in range(num_processes):
        Uc_hatT[:, :, i*Np:(i+1)*Np] = U_mpi[i]
    u[:] = irfft2(Uc_hatT, axes=(2,1))
    return u
\end{python}
Consider the function \inpyth{fftn_mpi}. In the first call to \inpyth{rfft2} each processor performs a complete two dimensional FFT in both $z$ and $y$ directions on the original real data structure \inpyth{u}. The first transform, in the $z$-direction, is real to complex and the second complex to complex. The real to complex transform reduces the size of the data structure to the one seen in Fig.~\ref{slabsubfig1}. To be able to perform the final transform in the $x$-direction, data must be communicated between all processors. The second index of the data structure ($y$-direction, see Fig.~\ref{slabsubfig2}) will now be shared amongst the processors. The transformation involving \inpyth{U_mpi} and \inpyth{Uc_hatT} performs the transformation from the data structure in Fig. \ref{slabsubfig1} to the one in Fig. \ref{slabsubfig2}. After the transformation the data structures have the correct shape, but they contain the wrong data. The communication of data required for the final transform takes place in one single MPI \inpyth{Alltoall} operation, where the \inpyth{mpitype} is a placeholder for \inpyth{MPI.F_DOUBLE_COMPLEX} or \inpyth{MPI.F_FLOAT_COMPLEX} depending on settings. After this communication the data is lined up for the final FFT in the $x$-direction. The \inpyth{ifftn_mpi} routine is basically the inverse of the forward transform and it should be straightforward to follow.

Three global pre-allocated complex work arrays are needed: \inpyth{Uc_hatT[Np, N, N/2+1]},  \inpyth{Uc_hat[N, Np, N/2+1]} and \inpyth{U_mpi[num_processes, Np, Np, N/2+1]}.
Note that  \inpyth{Alltoall} requires two work arrays as placeholders, one for each of the data structures seen in Fig \ref{fig:Slabdecomp} (b) and (c). Alternatively, the data transfer may be performed without explicit work arrays using the in-place \inpyth{Sendrecv_replace} along with transpose operations, as shown in alternative versions of these functions in the appendix.

\begin{figure}[t!]
\subfigure[Physical mesh]{
  \includegraphics[scale=0.2]{slabZ1.png}
  \label{slabsubfig0}
  }
\subfigure[Wavenumber mesh after real transform]{
  \includegraphics[scale=0.2]{slabZ2.png}
  \label{slabsubfig1}
}
\subfigure[Final wavenumber mesh.]{
  \includegraphics[scale=0.2]{slabZ3.png}
  \label{slabsubfig2}
  }
\caption{Slab decomposition of physical mesh \subref{subfig0}, intermediate wavenumber mesh \subref{subfig1}, and final wavenumber mesh \subref{subfig2}.  }
\label{fig:Slabdecomp}
\end{figure}

\subsection{2D Pencil decomposition}
\label{pencil2D}

For massively parallel simulations the slab decomposition falls short since the number of CPUs allowed is limited by $N$. Large-scale simulations using up to $N^2$ CPUs commonly employ the 2D pencil decomposition, first suggested by Ding, Ferraro and Gennery in 1995 \cite{Ding95}. Publically available implementations of the 3D parallel FFT that makes use of the pencil decomposition are the Parallel FFT Subroutine Library by \cite{PlimptonFFT}, the P3DFFT library by Pekurovsky \cite{p3dfft, Pekurovsky2012}, the 2DECOMP\&FFT library by Li and Laizet \cite{Li2010} and PFFT by Pippig \cite{Pi13}.

The 2D pencil decomposition strategy is illustrated in Fig. \ref{fig:Pencildecomp} for a box of size $16^3$, using 4 CPUs. The datastructures are split in the plane normal to the direction where we are performing the 1D FFTs. That is, for the physical mesh in Fig. \ref{fig:Pencildecomp} (a) the $x-y$ plane is split up in a $2\times2$ processor mesh. Each CPU may now perform 64 ($= 8 \times 8$) 1D real FFTs in the $z$-direction on its $8 \times 8 \times 16$ physical mesh. Afterwords, the complex data are laid out as shown in Fig. \ref{fig:Pencildecomp} (b). The second transform takes place in the $x$-direction, and for this to happen, data must be exchanged between processors 0 and 1 as well as 2 and 3. The datastructures must also be transformed to the shape seen in Fig. \ref{fig:Pencildecomp} (c). Each CPU may now perform the 32 ($4 \times 8$) individual 1D FFTs in the $x$-direction. The same procedure is followed to end up with datastructures aligned in the final $y$-direction, see Fig. \ref{fig:Pencildecomp} (d). However, this time processor 0 communicates with processor 2 and likewise 1 with 3. Evidently, each processor belongs to two different groups of communicators. One for communication in the $x-z$ plane (Fig. \ref{fig:Pencildecomp} (b) to (c)) and one for communication in the $x-y$ plane (Fig \ref{fig:Pencildecomp} (c) to (d)). The MPI communicator groups and the distributed mesh are created in Python as shown below:

%The Nyquist frequency is neglected for the pencil decomposition because with it the number of wavenumers in the z-direction is odd and not possible to share evenly between an even number of processors. Briefly, there are at least three different solutions to this problem, i) Neglect the Nyquist frequency, ii) Let one processor have responsibility for one more wavenumber than the others, iii) Place the real numbers of the Nyquist frequency in the complex part of the lowest wavenumber (also real) before transforming and communicating. The first strategy requires the least effort by far. The highest frequency (the Nyquist frequency $k=N/2+1$) is often neglected in turbulence simulations because ``this is a coefficient for a Fourier mode that is not carried in the Fourier representation of the solution'' \cite{Lee2013}. 

\begin{python}
num_processes = comm.Get_size()  # Total number of CPUs
rank = comm.Get_rank()           # Global rank
P1 = 2                           # CPUs in 1st direction
P2 = num_processes / P1          # CPUs in 2nd direction
N1 = N/P1
N2 = N/P2

# Create two communicator groups for each rank
commxz = comm.Split(rank%P1)
commxy = comm.Split(rank/P1)

xyrank = commxy.Get_rank() # Local rank in xy-plane
xzrank = commxz.Get_rank() # Local rank in xz-plane

# Create the physical mesh
x1 = slice(xyrank * N1, (xyrank+1) * N1, 1)
x2 = slice(xzrank * N2, (xzrank+1) * N2, 1)
X = mgrid[x1, x2, :N].astype(float)*L/N
\end{python}
With reference to Fig. \ref{fig:Pencildecomp}, the two communicator groups for CPU with global rank 0 is \inpyth{commxz} = [0, 1] and \inpyth{commxy} = [0, 2].

\begin{figure}
\subfigure[Physical mesh]{
  \includegraphics[scale=0.22]{pencil0.png}
  \label{subfig0}
  }
\subfigure[Wavenumber mesh after real transform]{
  \includegraphics[scale=0.22]{pencil1.png}
  \label{subfig1}
}
\subfigure[Intermediate wavenumber mesh]{
  \includegraphics[scale=0.22]{pencil3.png}
  \label{subfig2}
  }
\subfigure[Final wavenumber mesh.]{
  \includegraphics[scale=0.22]{pencil2.png}
  \label{subfig3}
  }
\caption{2D pencil decomposition of physical mesh \subref{subfig0} and the three wavenumber meshes \subref{subfig1}, \subref{subfig2}, \subref{subfig3}. The decomposition shown uses 4 CPUs, two in each direction normal to the direction of the current one-dimensional FFT. The FFT in the z-direction transforms the real data in \subref{subfig0} to the complex data in \subref{subfig1}. The data is then transformed and communicated to the composition seen in \subref{subfig2}. Here the FFT in $x$-direction is carried out before the data is transformed again and communicated to \subref{subfig3}, where the final FFT is performed.}
\label{fig:Pencildecomp}
\end{figure}

The entire 9 lines of code for the Python implementation of the 3D parallel FFT with the 2D pencil decomposition appear below.
The work arrays \inpyth{Uc_hat_y, Uc_hat_x, Uc_hat_z} are laid out as seen in Fig. \ref{fig:Pencildecomp} (d), (c) and (b) respectively. The array \inpyth{Uc_hat_xr} is a copy of \inpyth{Uc_hat_x} used only for communication. Note that all MPI communications take place with the date aligned in the $x$-direction, as shown in Fig. \ref{fig:Pencildecomp} (c), because this requires no preparation for the Alltoall call. The first index is automatically split up and distributed.

\begin{python}
def fftn_mpi(u, fu):
    """fft in three directions using mpi
    """    
    # Do fft in z direction on owned data
    Uc_hat_z[:] = rfft(u, axis=2)
    
    # Transform to x direction neglecting k=N/2 (Nyquist)
    for i in range(P1):
        Uc_hat_x[i*N1:(i+1)*N1] = Uc_hat_z[:, :, i*N1/2:(i+1)*N1/2]
    
    # Communicate and do fft in x-direction
    commyz.Alltoall([Uc_hat_x, mpitype], [Uc_hat_xr, mpitype])
    Uc_hat_x[:] = fft(Uc_hat_xr, axis=0)        
    
    # Communicate and transform to final z-direction
    commxz.Alltoall([Uc_hat_x, mpitype], [Uc_hat_xr, mpitype])    
    for i in range(P2): 
        Uc_hat_y[:, i*N2:(i+1)*N2] = Uc_hat_xr[i*N2:(i+1)*N2]
                                   
    # Do fft for last direction 
    fu[:] = fft(Uc_hat_y, axis=1)
    return fu
\end{python}
Note that data communication is performed when the data are aligned with the $x$-direction (Fig. \ref{fig:Pencildecomp} (c)). Here the datastructure \inpyth{Uc_hat_x} is of shape \inpyth{(N, N1/2, N2)}, where \inpyth{N1=N/P1} and \inpyth{N2=N/P2} and \inpyth{P1} and \inpyth{P2} are the division of processors used for the two split directions. In Fig. \ref{fig:Pencildecomp} \inpyth{P1} = \inpyth{P2} = 2. In the call to \inpyth{Alltoall} the first dimension of \inpyth{Uc_hat_x}, i.e. $N$, is automatically broadcasted such that \inpyth{Uc_hat_x} gets the shape \inpyth{(P1, N1, N2, N1/2)}.

\section{Performance}
Our Python based DNS solver has been tested on SHAHEEN, at the KAUST supercomputing laboratory. The primary computational resource of Shaheen consists of 16 racks of Blue Gene/P. Each rack contains 1024 quad-core, 32-bit, 850 MHz PowerPC compute nodes, for a total of 65,536 compute cores. Each node is equipped with 4GB of system memory, providing a total 64TB of distributed memory across the resource. Blue Gene nodes are interconnected by a three-dimensional point-to-point "torus" network used for general-purpose message-passing and multicast communication. By using many small, low-power, densely packaged chips, Blue Gene/P exceeded the power efficiency of other supercomputers of its generation, and at 371 MFLOPS/W Blue Gene/P installations ranked at or near the top of the Green500 lists in 2007-2008 \cite{top500green}.  Per November 2009, the TOP500 list contained 15 Blue Gene/P installations of 2-racks (2048 nodes, 8192 processor cores, 23.86 TFLOPS Linpack) and larger \cite{top500}.

It is customary for pseudospectral solvers to assume that the Fourier transforms are mainly responsible for the computational cost, and as such CPU time should ideally be scaling like

\begin{equation}
 \frac{N^3 \log_2 N}{M},
\end{equation}
where $M$ is the number of CPUs. In other words, strong scaling implies CPU time per time step proportional to $1/M$, whereas weak scaling, where $N^3/M$ is kept constant, should ideally show CPU times proportional to $\log_2 N$. A more thorough analysis of scalability factors for a 3D FFT using the pencil domain decomposition has been given by Ayala and Wang \cite{ayala2013}.

\subsection{Dynamic loading of Python}
Running a Python solver on a supercomputer presents a few challenges. Most importantly, the dynamic loading time of Python, i.e., the time to simply get started, may become unreasonably high. To load Python on the complete 16 racks of SHAHEEN takes approximately 45 minutes. The main reason is that large-scale supercomputers, such as Shaheen, make use of parallel file systems designed to support large distributed loads, where each CPU is writing or reading data independently. With dynamic loading, however, each process attempts to access the same files simultaneously and bottlenecks appear. 

A few solutions to the loading problem are known. The \inpyth{MPI_Import} Python module of Langton \cite{mpi_import} is designed to overload the built in import mechanism of Python such that only one rank (the zero rank) searches the filesystem for a module and then subsequently broadcasts the results to the remaining ranks. A drawback is the Python implementation where several modules \inpyth{(sys, imp, __builtin__,types, mpi4py)} need to be imported through the regular Python import mechanism. A second approach of Langton is to rewrite the regular finders and loaders modules and lets one rank do all the initial work, caching the modules it finds. 

A better solution is provided through the Scalable Python implementation \cite{scalablepython, Enkovaara201117}. Here CPython is modified slightly such that during import operations only a single process performs the actual I/O, and MPI is used for broadcasting the data to other MPI tasks. The solution provided by scalable python is used in this work and as such dynamic loading times are kept very low (approximately 30 seconds for a full rack).

\subsection{Verification}
To verify our Python based DNS solver we run the Taylor Green test case for a Reynolds number of 1600. This well known test case is used, e.g., by the annual Workshop on High-Order CFD Methods, that provides temporal evolution of reference data (https://www.grc.nasa.gov/wp-content/uploads/sites/22/ C3.3$\_$datafiles.zip) for kinetic energy, energy dissipation and vorticity in the computational domain.

The Taylor Green vortex is initialized as
\begin{python}
nu = 1./1600
dt = 0.0001
U[0] = sin(X[0])*cos(X[1])*cos(X[2])
U[1] =-cos(X[0])*sin(X[1])*cos(X[2])
U[2] = 0 
\end{python}
for both the slab and the pencil decomposition. The Reynolds number is fixed at 1600 by setting the viscosity coefficient to 1./1600 and the time step is fixed at 0.0001. We use a real mesh of size $N=512$, since that matches the reference data. A resolution of $512^3$ is reported by by Bratchet \cite{brachet1991direct} to represent a fully resolved DNS.

The code is run on Shaheen using 512 cores. The entire simulation is 20 seconds real time using 200,000 time steps. The simulation takes blah hours and the results are nearly inseparable from the reference solutions.

\subsection{Weak scaling}

\subsection{Strong scaling}

\subsection{Visualization}

\section{Conclusions}

\newpage
\appendix

\begin{python}
def fftn_mpi(u, fu):
    """FFT in 3D using an alternative MPI implementation."""
    # Do FFT for two directions, first real
    ft = fu.transpose(2,1,0)
    ft[:] = rfft2(u, axes=(2,1))

    # Create transformed view of fu array
    fu_send = fu.reshape((num_processes, Np, Nf, Np))

    # Communicate
    for i in range(num_processes):
        if not i == rank:
            comm.Sendrecv_replace([fu_send[i], mpitype], i, 0, i, 0)

    # Align data with final x-direction
    fu_send[:] = fu_send.transpose(0,3,2,1)

    # Do FFT for last direction
    fu[:] = fft(fu, axis=0)
    return fu

def ifftn_mpi(fu, u):
    """ifft in 3D using an alternative MPI implementation."""
    # Do inverse FFT of first owned direction
    Uc_hat[:] = ifft(fu, axis=0)

    # Create transformed view of Uc_hat
    Uc_send = Uc_hat.reshape((num_processes, Np, Nf, Np))

    # Communicate and transform
    for i in range(num_processes):
       if not i == rank:
           comm.Sendrecv_replace([Uc_send[i], mpitype], i, 0, i, 0)
       Uc_hatT[:, :, i*Np:(i+1)*Np] = Uc_send[i]

    # Do inverse FFT for last two directions, final real
    u[:] = irfft2(Uc_hatT, axes=(2,1))
    return u
\end{python}
Note that no explicit work arrays are used with the forward transform.

\bibliography{bib.bib}


\end{document}
